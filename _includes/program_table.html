<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-f7xd{background-color:#CECECE;border-color:inherit;color:#333;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-kr4b{background-color:#FFF;border-color:inherit;color:#333;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-acii{background-color:#FFF;border-color:inherit;color:#333;text-align:left;vertical-align:top}
.tg .tg-prhl{background-color:#FFF;border-color:inherit;color:#1f2328;text-align:left;text-decoration:underline;vertical-align:top}
.tg .tg-07pr{background-color:#FFF;border-color:inherit;color:#1F2328;text-align:left;vertical-align:top}
.tg .tg-e4b2{background-color:#FFF;border-color:inherit;color:#1F1F1F;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 100%">
<colgroup>
<col style="width: 7%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 13%">
</colgroup>
<thead>
  <tr>
    <th class="tg-f7xd"><span style="font-weight:bold">Date</span></th>
    <th class="tg-f7xd"><span style="font-weight:bold">Title</span></th>
    <th class="tg-f7xd"><span style="font-weight:bold">Speaker</span></th>
    <th class="tg-f7xd"><span style="font-weight:bold">Content</span></th>
    <th class="tg-f7xd"><span style="font-weight:bold">Links</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">26.02</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Intro and Mathematical foundation to DL</span></td>
    <td class="tg-e4b2"><span style="font-weight:normal;color:#1F1F1F">Bartek Wilczynski (Warsaw)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-prhl"><a href="https://github.com/deeplife4eu/Lecture-materials">Lecture slides</a>, <a href="https://github.com/deeplife4eu/Lecture-materials">Practical session</a>
</td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">04.03</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Convolutional and Recurrent neural networks</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Marco Frasca (Milano)</span></td>
    <td class="tg-acii"> Shift-invariance problem, convolution operation, convolutional layers, distributed convolution, pooling, padding. Sketch of some well-known convolutional architectures. Temporally related inputs, recurrent neaural networks, long short term memories.</td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">11.03</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Autoencoders and variational autoencoders</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Carl Herrmann (Heidelberg)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">18.03</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Attention mechanisms and transformers</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Dario Malchiodi (Milano)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii" colspan="5"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">08.04</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Transformers and RNN for sequence analysis</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Dario Malchiodi (Milano)</span></td>
    <td class="tg-acii">The lecture will analyze transformer models, considering NLP as an application field. Focusing on sequence prediction, the main ingredients of a transformer will be introduced and analyzed. In particular, the attention mechanism will be explained in all the variations used in a transformer (namely, self-attention, masked attention, and multi-head attention). Subsequently, the training process of a transformer will be considered, focusing on self-supervision, fine-tuning, and task-specific training components. Although the main content of the lecture will consider the encoder-decoder architecture, the variants encompassing only an encoder and only a decoder will be introduced. Finally, some extensions of the transformer architecture going beyond NLP will be briefly touched.</td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">15.04</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Models for multimodal data integration</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Britta Velten (Heidelberg)</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">This lecture will provide an overview of the basic statistical concepts that are important for the joint analysis of multi-modal data. We will discuss statistical properties of multi-omics data and challenges, followed by an overview on different strategies for supervised and unsupervised integration of multi-omics data and a deep dive into MOFA as example for an unsupervised method. We will discuss the underlying probabilistic model and explore different downstream analyses that can help to interpret the results of the method. We will illustrate the method on case studies.</span></td>
    <td class="tg-07pr"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">22.04</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">VAE in single-cell genomics</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Carl Herrmann (Heidelberg)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii" colspan="5"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">29.04</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">AF, EMSFold to predict structure of proteins</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Joanna Sulkowska (Warsaw)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">06.05</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Deep learning models for protein-ligand binding site prediction</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Joanna Sulkowska (Warsaw)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">13.05</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">RNN, CNN models for topology/graph analysis in biopolymers</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">David Hoksza (Prag)</span></td>
    <td class="tg-acii">We will introduce methods for protein-small molecule binding site prediction focusing on the structure-based methods. We will go along the timeline by introducing traditional non-ML-based methods, followed by methods using traditional ML techniques, such as Random Forests, and continuing with deep-learning approaches, such as CNN, and conclude with the new kid on the block, protein language models.</td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">23.05</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Diffusion models for protein design</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Elodie Laine (Paris)</span></td>
    <td class="tg-acii">Unlock the potential of generative models based on diffusion for de novo protein design. Explore their flexibility in shaping proteins according to desired shapes, functions, and active sites. This course provides a historical overview, operational insights, and showcases recent applications. Delve into challenges in discrete vs. continuous spaces and conclude with a comparative analysis against simpler generative models.</td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii" colspan="5"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">27.05</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Intro to BioImage Analysis and Deep Learning Utilization</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Martin Schatz (Prag)</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Introduction in the field of BioImage Analysis with an Deep Learning Utilization focus. We will discuss challanges of BioImage Analysis, you will learn about Community-Driven Resource not only for an Accessible Deep Learning. The hands on part will focus on knowlege and experience for utilization of Noise2Void and Stardist deep learning models.</span></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">03.06</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Deep Architectures for sampling macromolecules</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Grégoire Sergeant-Perthuis (Paris)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">10.06</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Deep learning for segmentation</span></td>
    <td class="tg-acii"><span style="color:#333;background-color:#FFF">Karl Rohr (Heidelberg)</span></td>
    <td class="tg-acii"></td>
    <td class="tg-acii"></td>
  </tr>
  <tr>
    <td class="tg-acii" colspan="5"></td>
  </tr>
  <tr>
    <td class="tg-kr4b"><span style="font-weight:bold">14-16.06</span></td>
    <td class="tg-kr4b"><span style="font-weight:bold">Final meeting Heidelberg</span></td>
    <td class="tg-acii"></td>
    <td class="tg-0pky"><span style="color:#333;background-color:#FFF">The course will end with a 2-day workshop and hackathon meeting in Heidelberg during which students will be able to implement a short project and listen to scientific lectures.
<br>
Feel free to post your project ideas <a href="https://github.com/orgs/deeplife4eu/discussions/categories/ideas-for-hackathon-projects)">here</a>!</span></td>
    <td class="tg-0pky"></td>
  </tr>
</tbody>
</table>
